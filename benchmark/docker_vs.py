#!/usr/bin/env python3
"""
SkillLite æ€§èƒ½åŸºå‡†æµ‹è¯•: Skillbox (Rust æ²™ç®±) vs Docker vs åŸç”Ÿ Python

æµ‹è¯•ç»´åº¦ï¼š
1. å†·å¯åŠ¨æ—¶é—´ - ä»å¯åŠ¨åˆ°æ‰§è¡Œç¬¬ä¸€è¡Œä»£ç çš„æ—¶é—´
2. ä»£ç æ‰§è¡Œæ—¶é—´ - è¿è¡Œç›¸åŒä»£ç çš„æ€»æ—¶é—´
3. å¹¶å‘æ€§èƒ½ - åŒæ—¶å¯åŠ¨å¤šä¸ªå®ä¾‹çš„è¡¨ç°
"""

import time
import subprocess
import statistics
import json
import os
import tempfile
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed


def check_command_available(command: str) -> bool:
    """æ£€æŸ¥å‘½ä»¤æ˜¯å¦å¯ç”¨"""
    return shutil.which(command) is not None


def check_docker_available() -> bool:
    """æ£€æŸ¥ Docker æ˜¯å¦å¯ç”¨"""
    if not check_command_available("docker"):
        return False
    try:
        result = subprocess.run(
            ["docker", "version"],
            capture_output=True,
            timeout=10
        )
        return result.returncode == 0
    except (FileNotFoundError, subprocess.TimeoutExpired):
        return False


def check_skillbox_available(binary_path: str = None) -> tuple:
    """æ£€æŸ¥ skillbox æ˜¯å¦å¯ç”¨ï¼Œè¿”å› (æ˜¯å¦å¯ç”¨, å®é™…è·¯å¾„)"""
    # ä¼˜å…ˆä½¿ç”¨æŒ‡å®šè·¯å¾„
    if binary_path and os.path.exists(binary_path):
        try:
            subprocess.run([binary_path, "--help"], capture_output=True, timeout=10)
            return True, binary_path
        except Exception:
            pass
    
    # æ£€æŸ¥ç³»ç»Ÿ PATH
    system_path = shutil.which("skillbox")
    if system_path:
        return True, system_path
    
    # æ£€æŸ¥é¡¹ç›®ç›®å½•
    project_paths = [
        "./skillbox/target/release/skillbox",
        "../skillbox/target/release/skillbox",
        os.path.expanduser("~/.cargo/bin/skillbox"),
    ]
    for path in project_paths:
        if os.path.exists(path):
            return True, path
    
    return False, ""


class SkillboxBenchmark:
    """SkillLite Rust æ²™ç®± (skillbox) æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, binary_path: str, work_dir: str = None):
        self.binary_path = binary_path
        self.work_dir = work_dir or tempfile.mkdtemp(prefix="skillbox_bench_")
        self._setup_test_skill()
    
    def _setup_test_skill(self):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„ Skill ç›®å½•ç»“æ„"""
        self.skill_dir = os.path.join(self.work_dir, "test-skill")
        scripts_dir = os.path.join(self.skill_dir, "scripts")
        os.makedirs(scripts_dir, exist_ok=True)
        
        # åˆ›å»º SKILL.md
        skill_md = """---
name: test-skill
description: Benchmark test skill
version: 1.0.0
entry_point: scripts/main.py
---
# Test Skill
"""
        with open(os.path.join(self.skill_dir, "SKILL.md"), "w") as f:
            f.write(skill_md)
    
    def _create_test_script(self, code: str) -> str:
        """åˆ›å»ºæµ‹è¯•è„šæœ¬å¹¶è¿”å›è·¯å¾„"""
        script_path = os.path.join(self.skill_dir, "scripts", "main.py")
        with open(script_path, "w") as f:
            f.write(code)
        return script_path
    
    def measure_startup(self, iterations: int = 10) -> list:
        """æµ‹é‡å¯åŠ¨æ—¶é—´ï¼ˆæ‰§è¡Œæœ€ç®€å•çš„ä»£ç ï¼‰"""
        times = []
        self._create_test_script('import json; print(json.dumps({"result": "hello"}))')
        input_json = '{}'  # ç©ºçš„è¾“å…¥ JSON
        
        for i in range(iterations):
            start = time.perf_counter()
            result = subprocess.run(
                [self.binary_path, "run", self.skill_dir, input_json],
                capture_output=True,
                timeout=30,
                cwd=self.work_dir
            )
            end = time.perf_counter()
            elapsed = (end - start) * 1000
            times.append(elapsed)
            
            if result.returncode != 0 and i == 0:
                print(f"    è­¦å‘Š: skillbox è¿”å›éé›¶é€€å‡ºç : {result.returncode}")
                stderr = result.stderr.decode() if result.stderr else ""
                if stderr:
                    print(f"    stderr: {stderr[:200]}")
        
        return times
    
    def measure_execution(self, code: str, iterations: int = 10) -> list:
        """æµ‹é‡ä»£ç æ‰§è¡Œæ—¶é—´"""
        times = []
        self._create_test_script(code)
        input_json = '{}'
        
        for _ in range(iterations):
            start = time.perf_counter()
            subprocess.run(
                [self.binary_path, "run", self.skill_dir, input_json],
                capture_output=True,
                timeout=60,
                cwd=self.work_dir
            )
            end = time.perf_counter()
            times.append((end - start) * 1000)
        
        return times
    
    def measure_concurrent(self, num_concurrent: int = 5, iterations: int = 3) -> dict:
        """æµ‹é‡å¹¶å‘æ‰§è¡Œæ€§èƒ½"""
        self._create_test_script('import json; print(json.dumps({"result": "concurrent test"}))')
        input_json = '{}'
        
        def run_once():
            start = time.perf_counter()
            subprocess.run(
                [self.binary_path, "run", self.skill_dir, input_json],
                capture_output=True,
                timeout=30,
                cwd=self.work_dir
            )
            return (time.perf_counter() - start) * 1000
        
        all_times = []
        for _ in range(iterations):
            with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
                futures = [executor.submit(run_once) for _ in range(num_concurrent)]
                batch_times = [f.result() for f in as_completed(futures)]
                all_times.extend(batch_times)
        
        return {
            "mean": statistics.mean(all_times),
            "max": max(all_times),
            "total_runs": len(all_times),
        }
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶ç›®å½•"""
        if self.work_dir and os.path.exists(self.work_dir) and "skillbox_bench_" in self.work_dir:
            shutil.rmtree(self.work_dir, ignore_errors=True)


class DockerBenchmark:
    """Docker æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, image: str = "python:3.11-slim"):
        self.image = image
        self._ensure_image()
    
    def _ensure_image(self):
        """ç¡®ä¿é•œåƒå­˜åœ¨"""
        print(f"  æ­£åœ¨æ£€æŸ¥ Docker é•œåƒ {self.image}...")
        result = subprocess.run(
            ["docker", "images", "-q", self.image],
            capture_output=True,
            timeout=30
        )
        if not result.stdout.strip():
            print(f"  æ­£åœ¨æ‹‰å–é•œåƒ...")
            subprocess.run(["docker", "pull", self.image], capture_output=True, timeout=300)
    
    def measure_startup(self, iterations: int = 10) -> list:
        """æµ‹é‡å®¹å™¨å¯åŠ¨æ—¶é—´"""
        times = []
        test_code = 'print("hello")'
        
        for _ in range(iterations):
            start = time.perf_counter()
            subprocess.run(
                ["docker", "run", "--rm", self.image, "python", "-c", test_code],
                capture_output=True,
                timeout=60
            )
            end = time.perf_counter()
            times.append((end - start) * 1000)
        
        return times
    
    def measure_execution(self, code: str, iterations: int = 10) -> list:
        """æµ‹é‡ä»£ç æ‰§è¡Œæ—¶é—´"""
        times = []
        
        for _ in range(iterations):
            start = time.perf_counter()
            subprocess.run(
                ["docker", "run", "--rm", self.image, "python", "-c", code],
                capture_output=True,
                timeout=120
            )
            end = time.perf_counter()
            times.append((end - start) * 1000)
        
        return times
    
    def measure_concurrent(self, num_concurrent: int = 5, iterations: int = 3) -> dict:
        """æµ‹é‡å¹¶å‘æ‰§è¡Œæ€§èƒ½"""
        def run_once():
            start = time.perf_counter()
            subprocess.run(
                ["docker", "run", "--rm", self.image, "python", "-c", 'print("concurrent")'],
                capture_output=True,
                timeout=60
            )
            return (time.perf_counter() - start) * 1000
        
        all_times = []
        for _ in range(iterations):
            with ThreadPoolExecutor(max_workers=num_concurrent) as executor:
                futures = [executor.submit(run_once) for _ in range(num_concurrent)]
                batch_times = [f.result() for f in as_completed(futures)]
                all_times.extend(batch_times)
        
        return {
            "mean": statistics.mean(all_times),
            "max": max(all_times),
            "total_runs": len(all_times),
        }


class NativePythonBenchmark:
    """åŸç”Ÿ Python æ€§èƒ½æµ‹è¯•ï¼ˆä½œä¸ºåŸºå‡†å‚ç…§ï¼‰"""
    
    def __init__(self):
        self.python_path = shutil.which("python3") or shutil.which("python")
    
    def measure_startup(self, iterations: int = 10) -> list:
        """æµ‹é‡åŸç”Ÿ Python å¯åŠ¨æ—¶é—´"""
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            subprocess.run(
                [self.python_path, "-c", 'print("hello")'],
                capture_output=True,
                timeout=30
            )
            end = time.perf_counter()
            times.append((end - start) * 1000)
        return times
    
    def measure_execution(self, code: str, iterations: int = 10) -> list:
        """æµ‹é‡ä»£ç æ‰§è¡Œæ—¶é—´"""
        times = []
        for _ in range(iterations):
            start = time.perf_counter()
            subprocess.run(
                [self.python_path, "-c", code],
                capture_output=True,
                timeout=60
            )
            end = time.perf_counter()
            times.append((end - start) * 1000)
        return times


def print_comparison_results(results: dict, test_cases: dict):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    print("\n" + "=" * 70)
    print("  å¯¹æ¯”ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    has_skillbox = bool(results.get("skillbox"))
    has_docker = bool(results.get("docker"))
    has_native = bool(results.get("native_python"))
    
    # è¡¨å¤´
    header = f"{'æµ‹è¯•é¡¹':<20}"
    if has_native:
        header += f"{'Python (ms)':<14}"
    if has_skillbox:
        header += f"{'Skillbox (ms)':<14}"
    if has_docker:
        header += f"{'Docker (ms)':<14}"
    if has_skillbox and has_docker:
        header += f"{'Skillbox ä¼˜åŠ¿':<14}"
    
    print(f"\n{header}")
    print("-" * len(header))
    
    # æ•°æ®è¡Œ
    all_tests = ["startup"] + list(test_cases.keys())
    for test_name in all_tests:
        row = f"{test_name:<20}"
        
        native_time = results["native_python"].get(test_name, {}).get("mean", 0) if has_native else 0
        skillbox_time = results["skillbox"].get(test_name, {}).get("mean", 0) if has_skillbox else 0
        docker_time = results["docker"].get(test_name, {}).get("mean", 0) if has_docker else 0
        
        if has_native and native_time:
            row += f"{native_time:<14.2f}"
        elif has_native:
            row += f"{'-':<14}"
            
        if has_skillbox and skillbox_time:
            row += f"{skillbox_time:<14.2f}"
        elif has_skillbox:
            row += f"{'-':<14}"
            
        if has_docker and docker_time:
            row += f"{docker_time:<14.2f}"
        elif has_docker:
            row += f"{'-':<14}"
        
        if has_skillbox and has_docker and skillbox_time and docker_time:
            speedup = docker_time / skillbox_time
            row += f"{speedup:.1f}x æ›´å¿«"
        
        print(row)
    
    # å…³é”®ç»“è®º
    print("\n" + "-" * 70)
    print("ğŸ“Š å…³é”®ç»“è®º:")
    
    if has_skillbox and has_native:
        skillbox_startup = results["skillbox"].get("startup", {}).get("mean", 0)
        native_startup = results["native_python"].get("startup", {}).get("mean", 0)
        if skillbox_startup and native_startup:
            overhead = skillbox_startup - native_startup
            overhead_pct = (overhead / native_startup) * 100 if native_startup else 0
            print(f"  â€¢ Skillbox æ²™ç®±å¼€é”€: +{overhead:.1f} ms (+{overhead_pct:.0f}%)")
    
    if has_skillbox and has_docker:
        skillbox_startup = results["skillbox"].get("startup", {}).get("mean", 0)
        docker_startup = results["docker"].get("startup", {}).get("mean", 0)
        if skillbox_startup and docker_startup:
            speedup = docker_startup / skillbox_startup
            print(f"  â€¢ Skillbox vs Docker å¯åŠ¨é€Ÿåº¦: {speedup:.1f}x æ›´å¿«")
            print(f"  â€¢ Docker å¯åŠ¨æ—¶é—´: {docker_startup:.0f} ms")
            print(f"  â€¢ Skillbox å¯åŠ¨æ—¶é—´: {skillbox_startup:.0f} ms")


def save_results(results: dict):
    """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
    output_file = "benchmark_results.json"
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def run_benchmark_suite(skillbox_binary: str = None, docker_image: str = "python:3.11-slim", iterations: int = 10):
    """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    print("=" * 70)
    print("  SkillLite æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("  Skillbox (Rust æ²™ç®±) vs Docker vs åŸç”Ÿ Python")
    print("=" * 70)
    
    # æ£€æŸ¥å¯ç”¨çš„æµ‹è¯•ç¯å¢ƒ
    docker_available = check_docker_available()
    skillbox_available, skillbox_path = check_skillbox_available(skillbox_binary)
    
    print("\n[ç¯å¢ƒæ£€æµ‹]")
    print("-" * 50)
    print(f"  Skillbox (Rust æ²™ç®±): {'âœ“ å¯ç”¨ (' + skillbox_path + ')' if skillbox_available else 'âœ— ä¸å¯ç”¨'}")
    print(f"  Docker:               {'âœ“ å¯ç”¨' if docker_available else 'âœ— ä¸å¯ç”¨'}")
    print(f"  åŸç”Ÿ Python:          âœ“ å¯ç”¨ (ä½œä¸ºåŸºå‡†å‚ç…§)")
    
    if not skillbox_available and not docker_available:
        print("\nâš ï¸  è­¦å‘Š: Skillbox å’Œ Docker éƒ½ä¸å¯ç”¨")
        print("  å°†ä»…è¿è¡ŒåŸç”Ÿ Python åŸºå‡†æµ‹è¯•ä½œä¸ºå‚ç…§")
        print("\n  è¦è¿›è¡Œå®Œæ•´å¯¹æ¯”æµ‹è¯•ï¼Œè¯·ç¡®ä¿:")
        print("    1. ç¼–è¯‘ skillbox: cd skillbox && cargo build --release")
        print("    2. æˆ–å®‰è£… Docker: https://docs.docker.com/get-docker/")
    
    # æµ‹è¯•ç”¨ä¾‹ - Skillbox éœ€è¦ JSON è¾“å‡ºï¼Œæ‰€ä»¥ä½¿ç”¨ json.dumps
    test_cases = {
        "simple_print": 'import json; print(json.dumps({"result": "Hello, World!"}))',
        "loop_1000": 'import json; print(json.dumps({"result": sum(range(1000))}))',
        "loop_100000": 'import json; print(json.dumps({"result": sum(range(100000))}))',
        "string_ops": 'import json; print(json.dumps({"result": len("hello" * 1000)}))',
        "list_comprehension": 'import json; print(json.dumps({"result": len([x**2 for x in range(1000)])}))',
        "fibonacci": '''
import json
def fib(n):
    if n <= 1: return n
    return fib(n-1) + fib(n-2)
print(json.dumps({"result": fib(20)}))
''',
    }
    
    results = {"skillbox": {}, "docker": {}, "native_python": {}}
    
    # åŸç”Ÿ Python æµ‹è¯•ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
    print("\n[åŸç”Ÿ Python æµ‹è¯•] (æ— æ²™ç®±ï¼Œä½œä¸ºæ€§èƒ½åŸºå‡†)")
    print("-" * 50)
    native_bench = NativePythonBenchmark()
    
    print(f"  æµ‹è¯•å¯åŠ¨æ—¶é—´ ({iterations} æ¬¡)...")
    native_startup = native_bench.measure_startup(iterations)
    results["native_python"]["startup"] = {
        "mean": statistics.mean(native_startup),
        "min": min(native_startup),
        "max": max(native_startup),
    }
    print(f"    å¹³å‡å¯åŠ¨æ—¶é—´: {results['native_python']['startup']['mean']:.2f} ms")
    
    for name, code in test_cases.items():
        print(f"  æµ‹è¯• {name}...")
        exec_times = native_bench.measure_execution(code, iterations)
        results["native_python"][name] = {
            "mean": statistics.mean(exec_times),
            "min": min(exec_times),
            "max": max(exec_times),
        }
    
    # Skillbox æµ‹è¯•
    skillbox_bench = None
    if skillbox_available:
        print("\n[Skillbox æµ‹è¯•] (Rust åŸç”Ÿæ²™ç®±)")
        print("-" * 50)
        skillbox_bench = SkillboxBenchmark(skillbox_path)
        
        print(f"  æµ‹è¯•å¯åŠ¨æ—¶é—´ ({iterations} æ¬¡)...")
        try:
            skillbox_startup = skillbox_bench.measure_startup(iterations)
            results["skillbox"]["startup"] = {
                "mean": statistics.mean(skillbox_startup),
                "min": min(skillbox_startup),
                "max": max(skillbox_startup),
            }
            print(f"    å¹³å‡å¯åŠ¨æ—¶é—´: {results['skillbox']['startup']['mean']:.2f} ms")
            
            for name, code in test_cases.items():
                print(f"  æµ‹è¯• {name}...")
                exec_times = skillbox_bench.measure_execution(code, iterations)
                results["skillbox"][name] = {
                    "mean": statistics.mean(exec_times),
                    "min": min(exec_times),
                    "max": max(exec_times),
                }
            
            # å¹¶å‘æµ‹è¯•
            print(f"  æµ‹è¯•å¹¶å‘æ€§èƒ½ (5 å¹¶å‘)...")
            concurrent_result = skillbox_bench.measure_concurrent(num_concurrent=5, iterations=2)
            results["skillbox"]["concurrent_5"] = concurrent_result
            print(f"    å¹³å‡æ‰§è¡Œæ—¶é—´: {concurrent_result['mean']:.2f} ms")
            
        except Exception as e:
            print(f"    âŒ Skillbox æµ‹è¯•å¤±è´¥: {e}")
    else:
        print("\n[è·³è¿‡ Skillbox æµ‹è¯•]")
        print("  è¯·å…ˆç¼–è¯‘: cd skillbox && cargo build --release")
    
    # Docker æµ‹è¯•
    if docker_available:
        print("\n[Docker æµ‹è¯•]")
        print("-" * 50)
        docker_bench = DockerBenchmark(docker_image)
        
        print(f"  æµ‹è¯•å¯åŠ¨æ—¶é—´ ({iterations} æ¬¡)...")
        docker_startup = docker_bench.measure_startup(iterations)
        results["docker"]["startup"] = {
            "mean": statistics.mean(docker_startup),
            "min": min(docker_startup),
            "max": max(docker_startup),
        }
        print(f"    å¹³å‡å¯åŠ¨æ—¶é—´: {results['docker']['startup']['mean']:.2f} ms")
        
        for name, code in test_cases.items():
            print(f"  æµ‹è¯• {name}...")
            exec_times = docker_bench.measure_execution(code, iterations)
            results["docker"][name] = {
                "mean": statistics.mean(exec_times),
                "min": min(exec_times),
                "max": max(exec_times),
            }
        
        # å¹¶å‘æµ‹è¯•
        print(f"  æµ‹è¯•å¹¶å‘æ€§èƒ½ (5 å¹¶å‘)...")
        concurrent_result = docker_bench.measure_concurrent(num_concurrent=5, iterations=2)
        results["docker"]["concurrent_5"] = concurrent_result
        print(f"    å¹³å‡æ‰§è¡Œæ—¶é—´: {concurrent_result['mean']:.2f} ms")
    else:
        print("\n[è·³è¿‡ Docker æµ‹è¯• - Docker æœªå®‰è£…]")
    
    # æ¸…ç†
    if skillbox_bench:
        skillbox_bench.cleanup()
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print_comparison_results(results, test_cases)
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    return results


def measure_skillbox_cold_start(skillbox_path: str, iterations: int = 5):
    """æµ‹é‡ Skillbox å†·å¯åŠ¨æ—¶é—´ï¼ˆæ¸…é™¤ç³»ç»Ÿç¼“å­˜åé¦–æ¬¡æ‰§è¡Œï¼‰"""
    print("\n[Skillbox å†·å¯åŠ¨æµ‹è¯•]")
    print("-" * 50)
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
    work_dir = tempfile.mkdtemp(prefix="skillbox_cold_")
    skill_dir = os.path.join(work_dir, "test-skill")
    scripts_dir = os.path.join(skill_dir, "scripts")
    os.makedirs(scripts_dir, exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    with open(os.path.join(skill_dir, "SKILL.md"), "w") as f:
        f.write("---\nname: test\nversion: 1.0.0\nentry_point: scripts/main.py\n---\n")
    with open(os.path.join(scripts_dir, "main.py"), "w") as f:
        f.write('import json; print(json.dumps({"result": "cold start"}))')
    
    times = []
    
    for i in range(iterations):
        # å°è¯•æ¸…é™¤æ–‡ä»¶ç³»ç»Ÿç¼“å­˜ï¼ˆéœ€è¦ sudoï¼Œå¯èƒ½å¤±è´¥ï¼‰
        subprocess.run(["sync"], capture_output=True)
        subprocess.run(["sudo", "purge"], capture_output=True, timeout=10)
        
        start = time.perf_counter()
        subprocess.run(
            [skillbox_path, "run", skill_dir, "{}"],
            capture_output=True,
            timeout=30,
            cwd=work_dir
        )
        end = time.perf_counter()
        times.append((end - start) * 1000)
    
    # æ¸…ç†
    shutil.rmtree(work_dir, ignore_errors=True)
    
    print(f"  å¹³å‡å†·å¯åŠ¨æ—¶é—´: {statistics.mean(times):.2f} ms")
    print(f"  æœ€å¿«: {min(times):.2f} ms")
    print(f"  æœ€æ…¢: {max(times):.2f} ms")
    
    return times


def measure_docker_cold_start(image: str = "python:3.11-slim", iterations: int = 3):
    """æµ‹é‡ Docker çœŸæ­£çš„å†·å¯åŠ¨æ—¶é—´ï¼ˆæ¯æ¬¡éƒ½åˆ é™¤é•œåƒé‡æ–°æ‹‰å–ï¼‰"""
    print("\n" + "=" * 70)
    print("  å†·å¯åŠ¨å¯¹æ¯”æµ‹è¯•")
    print("  Skillbox vs Dockerï¼ˆæ¯æ¬¡åˆ é™¤é•œåƒåé‡æ–°æ‹‰å–ï¼‰")
    print("=" * 70)
    
    # å…ˆæµ‹è¯• Skillbox å†·å¯åŠ¨
    skillbox_available, skillbox_path = check_skillbox_available()
    skillbox_times = []
    if skillbox_available:
        skillbox_times = measure_skillbox_cold_start(skillbox_path, iterations=5)
    
    # Docker å†·å¯åŠ¨æµ‹è¯•
    print("\n[Docker å†·å¯åŠ¨æµ‹è¯•]")
    print("-" * 50)
    print("  âš ï¸  è¿™ä¸ªæµ‹è¯•ä¼šæ¯”è¾ƒæ…¢ï¼Œå› ä¸ºéœ€è¦é‡æ–°ä¸‹è½½é•œåƒ")
    
    docker_times = []
    
    for i in range(iterations):
        print(f"\n  ç¬¬ {i+1}/{iterations} æ¬¡å†·å¯åŠ¨æµ‹è¯•...")
        
        # 1. åˆ é™¤é•œåƒ
        print("    åˆ é™¤é•œåƒ...")
        subprocess.run(["docker", "rmi", "-f", image], capture_output=True, timeout=60)
        
        # 2. æ¸…ç† Docker ç¼“å­˜
        subprocess.run(["docker", "system", "prune", "-f"], capture_output=True, timeout=60)
        
        # 3. æµ‹é‡å†·å¯åŠ¨æ—¶é—´ï¼ˆåŒ…æ‹¬æ‹‰å–é•œåƒ + å¯åŠ¨å®¹å™¨ + æ‰§è¡Œä»£ç ï¼‰
        print("    å¼€å§‹å†·å¯åŠ¨è®¡æ—¶ï¼ˆåŒ…æ‹¬æ‹‰å–é•œåƒï¼‰...")
        start = time.perf_counter()
        result = subprocess.run(
            ["docker", "run", "--rm", image, "python", "-c", 'import json; print(json.dumps({"result": "cold start"}))'],
            capture_output=True,
            timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
        )
        end = time.perf_counter()
        
        elapsed = (end - start) * 1000
        docker_times.append(elapsed)
        print(f"    å†·å¯åŠ¨æ—¶é—´: {elapsed:.0f} ms ({elapsed/1000:.1f} ç§’)")
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š å†·å¯åŠ¨å¯¹æ¯”ç»“æœ:")
    print("=" * 70)
    
    if skillbox_times:
        skillbox_avg = statistics.mean(skillbox_times)
        print(f"\n  Skillbox:")
        print(f"    â€¢ å¹³å‡å†·å¯åŠ¨æ—¶é—´: {skillbox_avg:.0f} ms")
        print(f"    â€¢ èŒƒå›´: {min(skillbox_times):.0f} - {max(skillbox_times):.0f} ms")
    
    docker_avg = statistics.mean(docker_times)
    print(f"\n  Docker:")
    print(f"    â€¢ å¹³å‡å†·å¯åŠ¨æ—¶é—´: {docker_avg:.0f} ms ({docker_avg/1000:.1f} ç§’)")
    print(f"    â€¢ èŒƒå›´: {min(docker_times):.0f} - {max(docker_times):.0f} ms")
    
    if skillbox_times:
        speedup = docker_avg / skillbox_avg
        print(f"\n  ğŸš€ ç»“è®º:")
        print(f"    â€¢ Skillbox æ¯” Docker å†·å¯åŠ¨å¿« {speedup:.0f}x")
        print(f"    â€¢ Docker éœ€è¦ä¸‹è½½ ~150MB é•œåƒï¼ŒSkillbox æ˜¯æœ¬åœ°äºŒè¿›åˆ¶")
    
    return {"skillbox": skillbox_times, "docker": docker_times}


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description="SkillLite æ€§èƒ½åŸºå‡†æµ‹è¯•: Skillbox (Rust æ²™ç®±) vs Docker vs åŸç”Ÿ Python",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è‡ªåŠ¨æ£€æµ‹ skillboxï¼Œè·³è¿‡ Dockerï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
  python3 benchmark_comparison.py

  # æŒ‡å®š skillbox è·¯å¾„
  python3 benchmark_comparison.py --skillbox ./skillbox/target/release/skillbox

  # å®Œæ•´æµ‹è¯•ï¼ˆéœ€è¦ Dockerï¼‰
  python3 benchmark_comparison.py --iterations 20

  # ä½¿ç”¨ä¸åŒçš„ Docker é•œåƒ
  python3 benchmark_comparison.py --docker-image python:3.12-alpine

  # æµ‹è¯• Docker çœŸæ­£çš„å†·å¯åŠ¨ï¼ˆä¼šåˆ é™¤é•œåƒé‡æ–°æ‹‰å–ï¼Œè¾ƒæ…¢ï¼‰
  python3 benchmark_comparison.py --cold-start --iterations 3
"""
    )
    parser.add_argument(
        "--skillbox", 
        type=str, 
        default=None,
        help="Skillbox å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    parser.add_argument(
        "--docker-image", 
        type=str, 
        default="python:3.11-slim", 
        help="Docker é•œåƒåç§°ï¼ˆé»˜è®¤: python:3.11-slimï¼‰"
    )
    parser.add_argument(
        "--iterations", 
        type=int, 
        default=10, 
        help="æ¯ä¸ªæµ‹è¯•çš„è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤: 10ï¼‰"
    )
    parser.add_argument(
        "--cold-start",
        action="store_true",
        help="æµ‹è¯• Docker çœŸæ­£çš„å†·å¯åŠ¨ï¼ˆæ¯æ¬¡åˆ é™¤é•œåƒé‡æ–°æ‹‰å–ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.cold_start:
        # å†·å¯åŠ¨æµ‹è¯•æ¨¡å¼
        if not check_docker_available():
            print("é”™è¯¯: Docker æœªå®‰è£…æˆ–æœªè¿è¡Œ")
            exit(1)
        measure_docker_cold_start(args.docker_image, args.iterations)
    else:
        # æ­£å¸¸åŸºå‡†æµ‹è¯•
        run_benchmark_suite(
            skillbox_binary=args.skillbox,
            docker_image=args.docker_image,
            iterations=args.iterations
        )
